# ğŸš€ Guide d'ExÃ©cution des Pipelines HSE - AgenticX5

## Architecture Multi-Projets

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                        Ã‰COSYSTÃˆME AGENTICX5                                  â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                              â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                   â”‚
â”‚  â”‚   ZERVE.AI   â”‚    â”‚  SAFETY      â”‚    â”‚   CLAUDE     â”‚                   â”‚
â”‚  â”‚  (Notebooks) â”‚    â”‚  GRAPH DB    â”‚    â”‚  PROJECTS    â”‚                   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜                   â”‚
â”‚         â”‚                   â”‚                   â”‚                            â”‚
â”‚         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                            â”‚
â”‚                             â”‚                                                â”‚
â”‚                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”                                       â”‚
â”‚                    â”‚  HSE INGESTION  â”‚                                       â”‚
â”‚                    â”‚    PIPELINE     â”‚                                       â”‚
â”‚                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜                                       â”‚
â”‚                             â”‚                                                â”‚
â”‚         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                            â”‚
â”‚         â”‚                   â”‚                   â”‚                            â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”                     â”‚
â”‚  â”‚   BRONZE    â”‚    â”‚    SILVER    â”‚    â”‚    GOLD     â”‚                     â”‚
â”‚  â”‚  (Raw Data) â”‚â”€â”€â”€â–¶â”‚  (Cleaned)   â”‚â”€â”€â”€â–¶â”‚ (Harmonized)â”‚                     â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                     â”‚
â”‚                                                â”‚                             â”‚
â”‚                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”       â”‚
â”‚                    â”‚                           â”‚                     â”‚       â”‚
â”‚             â”Œâ”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”            â”Œâ”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”â”‚
â”‚             â”‚ PostgreSQL  â”‚            â”‚    Parquet   â”‚      â”‚   Neo4j     â”‚â”‚
â”‚             â”‚ Safety Graphâ”‚            â”‚  Data Lake   â”‚      â”‚ Knowledge   â”‚â”‚
â”‚             â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜            â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜â”‚
â”‚                                                                              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ“ Structure des Fichiers

```
agenticx5-hse-ingestion/
â”‚
â”œâ”€â”€ ğŸ“„ hse_data_ingestion.py      # Module principal d'ingestion
â”œâ”€â”€ ğŸ“„ config/
â”‚   â”œâ”€â”€ sources.yaml              # Configuration des sources
â”‚   â”œâ”€â”€ env.local.yaml            # Config environnement local
â”‚   â”œâ”€â”€ env.zerve.yaml            # Config Zerve.ai
â”‚   â””â”€â”€ env.production.yaml       # Config production
â”‚
â”œâ”€â”€ ğŸ“„ connectors/
â”‚   â”œâ”€â”€ kaggle_connector.py
â”‚   â”œâ”€â”€ osha_connector.py
â”‚   â”œâ”€â”€ eurostat_connector.py
â”‚   â”œâ”€â”€ ilostat_connector.py
â”‚   â””â”€â”€ cnesst_connector.py
â”‚
â”œâ”€â”€ ğŸ“„ pipelines/
â”‚   â”œâ”€â”€ bronze_pipeline.py
â”‚   â”œâ”€â”€ silver_pipeline.py
â”‚   â””â”€â”€ gold_pipeline.py
â”‚
â”œâ”€â”€ ğŸ“„ integrations/
â”‚   â”œâ”€â”€ postgresql_loader.py      # Chargement Safety Graph
â”‚   â”œâ”€â”€ neo4j_loader.py           # Chargement Knowledge Graph
â”‚   â””â”€â”€ parquet_exporter.py       # Export Data Lake
â”‚
â”œâ”€â”€ ğŸ“„ schedulers/
â”‚   â”œâ”€â”€ cron_scheduler.py
â”‚   â”œâ”€â”€ airflow_dag.py
â”‚   â””â”€â”€ prefect_flow.py
â”‚
â””â”€â”€ ğŸ“„ data/
    â”œâ”€â”€ bronze/                   # DonnÃ©es brutes
    â”œâ”€â”€ silver/                   # DonnÃ©es nettoyÃ©es
    â””â”€â”€ gold/                     # DonnÃ©es harmonisÃ©es
```

---

## ğŸ”§ Installation

### PrÃ©requis

```bash
# Python 3.10+
python --version

# CrÃ©er environnement virtuel
python -m venv venv
source venv/bin/activate  # Linux/Mac
# ou
.\venv\Scripts\activate   # Windows
```

### DÃ©pendances

```bash
# Installation des packages
pip install -r requirements.txt
```

**requirements.txt:**
```
pandas>=2.0.0
numpy>=1.24.0
requests>=2.28.0
pyarrow>=12.0.0
sqlalchemy>=2.0.0
psycopg2-binary>=2.9.0
kaggle>=1.5.0
python-dotenv>=1.0.0
pyyaml>=6.0
schedule>=1.2.0
neo4j>=5.0.0
openpyxl>=3.1.0
lxml>=4.9.0
tqdm>=4.65.0
```

### Variables d'Environnement

```bash
# .env file
# === API Keys ===
KAGGLE_USERNAME=your_kaggle_username
KAGGLE_KEY=your_kaggle_api_key
BLS_API_KEY=your_bls_api_key

# === Database ===
POSTGRES_HOST=localhost
POSTGRES_PORT=5432
POSTGRES_DB=safety_graph
POSTGRES_USER=agenticx5
POSTGRES_PASSWORD=your_password

# === Neo4j ===
NEO4J_URI=bolt://localhost:7687
NEO4J_USER=neo4j
NEO4J_PASSWORD=your_password

# === Paths ===
DATA_DIR=/path/to/data
LOG_DIR=/path/to/logs
```

---

## ğŸ–¥ï¸ ExÃ©cution Locale (DÃ©veloppement)

### 1. Mode CLI Basique

```bash
# Lister toutes les sources disponibles
python hse_data_ingestion.py --list

# ExÃ©cuter une source spÃ©cifique
python hse_data_ingestion.py --source kaggle_osha_injuries

# ExÃ©cuter toutes les sources prioritÃ© 1 (critiques)
python hse_data_ingestion.py --all --priority 1

# ExÃ©cuter avec rapport
python hse_data_ingestion.py --all --priority 2 --report

# Fusionner les tables Gold
python hse_data_ingestion.py --merge
```

### 2. Mode Python Interactif

```python
from hse_data_ingestion import HSEPipelineOrchestrator, HSE_SOURCES

# Initialiser l'orchestrateur
orchestrator = HSEPipelineOrchestrator(data_dir="./data")

# ExÃ©cuter Kaggle OSHA
result = orchestrator.run_single("kaggle_osha_injuries")
print(f"Status: {result['status']}")
print(f"Rows ingested: {result['steps']['gold']['rows']}")

# ExÃ©cuter toutes les sources
results = orchestrator.run_all(priority_threshold=2)

# GÃ©nÃ©rer rapport
report = orchestrator.generate_report()

# Fusionner Gold tables
merged_path = orchestrator.merge_gold_tables()
```

### 3. Script de Test Rapide

```python
# test_ingestion.py
"""
Test rapide d'ingestion HSE
"""
import os
os.environ['DATA_DIR'] = './test_data'

from hse_data_ingestion import (
    HSEPipelineOrchestrator,
    KaggleConnector,
    OSHAConnector,
    EurostatConnector,
    HSE_SOURCES
)

def test_single_source():
    """Tester une source individuelle"""
    config = HSE_SOURCES["kaggle_osha_injuries"]
    connector = KaggleConnector(config, "./test_data")
    
    # Test fetch
    df = connector.fetch()
    print(f"âœ… Fetched: {len(df)} rows")
    
    # Test transform
    df_silver = connector.transform(df)
    print(f"âœ… Transformed: {len(df_silver)} rows")
    
    # Test harmonize
    df_gold = connector.harmonize(df_silver)
    print(f"âœ… Harmonized: {len(df_gold)} rows")
    print(f"   Columns: {list(df_gold.columns)}")

if __name__ == "__main__":
    test_single_source()
```

---

## ğŸ“Š ExÃ©cution dans Zerve.ai

### Configuration Zerve

Zerve.ai est votre plateforme Data Science prÃ©fÃ©rÃ©e. Voici comment y intÃ©grer les pipelines :

### 1. Notebook Zerve - Setup

```python
# Cell 1: Installation
!pip install kaggle pandas pyarrow requests openpyxl

# Cell 2: Configuration
import os
os.environ['KAGGLE_USERNAME'] = 'your_username'
os.environ['KAGGLE_KEY'] = 'your_key'
os.environ['DATA_DIR'] = '/workspace/data'

# Cell 3: Import du module
# Upload hse_data_ingestion.py dans Zerve
from hse_data_ingestion import HSEPipelineOrchestrator, HSE_SOURCES
```

### 2. Notebook Zerve - ExÃ©cution Interactive

```python
# Cell 4: Initialisation
orchestrator = HSEPipelineOrchestrator(data_dir="/workspace/data")

# Cell 5: Dashboard des sources
import pandas as pd

sources_df = pd.DataFrame([
    {
        "Source": key,
        "Nom": config.name,
        "Type": config.type,
        "Juridiction": config.jurisdiction,
        "PrioritÃ©": config.priority,
        "ActivÃ©": config.enabled
    }
    for key, config in HSE_SOURCES.items()
])

display(sources_df.sort_values("PrioritÃ©"))
```

### 3. Notebook Zerve - Pipeline Complet

```python
# Cell 6: ExÃ©cution Pipeline
from tqdm.notebook import tqdm

results = []
sources_to_run = [
    "kaggle_osha_injuries",
    "osha_inspections", 
    "eurostat_esaw",
    "dares_at"
]

for source in tqdm(sources_to_run, desc="Ingestion HSE"):
    result = orchestrator.run_single(source)
    results.append({
        "source": source,
        "status": result["status"],
        "rows": result.get("steps", {}).get("gold", {}).get("rows", 0)
    })

results_df = pd.DataFrame(results)
display(results_df)
```

### 4. Notebook Zerve - Visualisation

```python
# Cell 7: Visualisation des donnÃ©es Gold
import plotly.express as px

# Charger la table Gold fusionnÃ©e
gold_df = pd.read_parquet("/workspace/data/gold/hse_incidents_global.parquet")

# Distribution par juridiction
fig1 = px.pie(
    gold_df.groupby("jurisdiction").size().reset_index(name="count"),
    values="count",
    names="jurisdiction",
    title="Distribution des Incidents par Juridiction"
)
fig1.show()

# Tendances temporelles
if "year" in gold_df.columns:
    yearly = gold_df.groupby(["year", "jurisdiction"]).size().reset_index(name="incidents")
    fig2 = px.line(
        yearly,
        x="year",
        y="incidents",
        color="jurisdiction",
        title="Ã‰volution des Incidents par AnnÃ©e"
    )
    fig2.show()
```

### 5. Template Zerve Complet

```python
"""
==========================================================
ZERVE NOTEBOOK: HSE Data Ingestion Pipeline
AgenticX5 / Safety Graph
==========================================================
"""

# %% [markdown]
# # ğŸ”„ Pipeline d'Ingestion HSE
# 
# Ce notebook exÃ©cute le pipeline complet d'ingestion des donnÃ©es HSE 
# internationales vers Safety Graph.

# %% Setup
import os
import sys
import pandas as pd
import numpy as np
from datetime import datetime

# Configuration
os.environ['DATA_DIR'] = '/workspace/agenticx5/data'
os.environ['KAGGLE_USERNAME'] = os.getenv('KAGGLE_USERNAME', '')
os.environ['KAGGLE_KEY'] = os.getenv('KAGGLE_KEY', '')

# Import du module (doit Ãªtre uploadÃ© dans Zerve)
sys.path.append('/workspace/agenticx5')
from hse_data_ingestion import HSEPipelineOrchestrator, HSE_SOURCES

print(f"âœ… Setup completed at {datetime.now()}")
print(f"ğŸ“‚ Data directory: {os.environ['DATA_DIR']}")

# %% Initialisation
orchestrator = HSEPipelineOrchestrator(
    data_dir=os.environ['DATA_DIR']
)

# Afficher les sources
print("\nğŸ“‹ Sources HSE ConfigurÃ©es:")
for key, config in sorted(HSE_SOURCES.items(), key=lambda x: x[1].priority):
    emoji = "âœ…" if config.enabled else "âŒ"
    print(f"  {emoji} [{config.priority}] {key}: {config.name}")

# %% ExÃ©cution SÃ©lective
# Choisir les sources Ã  exÃ©cuter
SOURCES_TO_RUN = [
    "kaggle_osha_injuries",    # Kaggle - 1M+ records USA
    "eurostat_esaw",           # Eurostat - 27 pays EU  
    "ilostat_injuries",        # ILOSTAT - 180+ pays
]

results = []
for source in SOURCES_TO_RUN:
    print(f"\n{'='*60}")
    print(f"ğŸ”„ Processing: {source}")
    print(f"{'='*60}")
    
    result = orchestrator.run_single(source)
    results.append(result)
    
    if result["status"] == "success":
        print(f"âœ… Success: {result['steps']['gold']['rows']} rows")
    else:
        print(f"âŒ Failed: {result.get('error', 'Unknown error')}")

# %% Rapport
report = orchestrator.generate_report()

print(f"\n{'='*60}")
print("ğŸ“Š RAPPORT D'EXÃ‰CUTION")
print(f"{'='*60}")
print(f"Date: {report['execution_date']}")
print(f"Sources traitÃ©es: {report['total_sources']}")
print(f"SuccÃ¨s: {report['successful']}")
print(f"Ã‰checs: {report['failed']}")
print(f"Total rows: {report['total_rows_ingested']:,}")

# %% Fusion Gold
print("\nğŸ”— Fusion des tables Gold...")
merged_path = orchestrator.merge_gold_tables()

if merged_path:
    gold_df = pd.read_parquet(merged_path)
    print(f"âœ… Table fusionnÃ©e: {len(gold_df):,} rows")
    print(f"ğŸ“Š Colonnes: {list(gold_df.columns)}")
    display(gold_df.head(10))
```

---

## ğŸ˜ Chargement dans PostgreSQL (Safety Graph)

### Script de Chargement

```python
# postgresql_loader.py
"""
Chargement des donnÃ©es Gold vers PostgreSQL Safety Graph
"""

import pandas as pd
from sqlalchemy import create_engine, text
from pathlib import Path
import os

class SafetyGraphLoader:
    """Chargeur pour PostgreSQL Safety Graph"""
    
    def __init__(self):
        self.engine = create_engine(
            f"postgresql://{os.getenv('POSTGRES_USER')}:"
            f"{os.getenv('POSTGRES_PASSWORD')}@"
            f"{os.getenv('POSTGRES_HOST')}:"
            f"{os.getenv('POSTGRES_PORT')}/"
            f"{os.getenv('POSTGRES_DB')}"
        )
    
    def create_tables(self):
        """CrÃ©er les tables si elles n'existent pas"""
        ddl = """
        -- Table principale des incidents HSE
        CREATE TABLE IF NOT EXISTS hse_incidents_global (
            id SERIAL PRIMARY KEY,
            incident_id VARCHAR(100),
            source VARCHAR(200),
            jurisdiction VARCHAR(50),
            incident_date DATE,
            year INTEGER,
            industry_code VARCHAR(20),
            industry_code_system VARCHAR(20),
            industry_name TEXT,
            establishment_size VARCHAR(50),
            incident_type VARCHAR(50),
            severity VARCHAR(50),
            nature_of_injury TEXT,
            body_part VARCHAR(100),
            event_type VARCHAR(200),
            days_lost NUMERIC,
            worker_age NUMERIC,
            worker_gender VARCHAR(20),
            narrative TEXT,
            latitude NUMERIC,
            longitude NUMERIC,
            created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
            updated_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
            
            -- Index pour les requÃªtes frÃ©quentes
            CONSTRAINT unique_incident UNIQUE (incident_id, source)
        );
        
        -- Index
        CREATE INDEX IF NOT EXISTS idx_incidents_jurisdiction ON hse_incidents_global(jurisdiction);
        CREATE INDEX IF NOT EXISTS idx_incidents_year ON hse_incidents_global(year);
        CREATE INDEX IF NOT EXISTS idx_incidents_industry ON hse_incidents_global(industry_code);
        CREATE INDEX IF NOT EXISTS idx_incidents_type ON hse_incidents_global(incident_type);
        
        -- Table de mÃ©tadonnÃ©es des sources
        CREATE TABLE IF NOT EXISTS hse_data_sources (
            id SERIAL PRIMARY KEY,
            source_key VARCHAR(100) UNIQUE,
            source_name VARCHAR(200),
            source_type VARCHAR(50),
            jurisdiction VARCHAR(50),
            url TEXT,
            last_ingestion TIMESTAMP,
            rows_ingested INTEGER,
            status VARCHAR(20)
        );
        """
        
        with self.engine.connect() as conn:
            for statement in ddl.split(';'):
                if statement.strip():
                    conn.execute(text(statement))
            conn.commit()
        
        print("âœ… Tables crÃ©Ã©es/vÃ©rifiÃ©es")
    
    def load_gold_data(self, parquet_path: str, if_exists: str = "append"):
        """Charger les donnÃ©es Gold dans PostgreSQL"""
        df = pd.read_parquet(parquet_path)
        
        # Renommer les colonnes pour PostgreSQL
        column_mapping = {
            'date': 'incident_date'
        }
        df = df.rename(columns=column_mapping)
        
        # Charger
        df.to_sql(
            'hse_incidents_global',
            self.engine,
            if_exists=if_exists,
            index=False,
            method='multi',
            chunksize=10000
        )
        
        print(f"âœ… ChargÃ© {len(df):,} rows dans hse_incidents_global")
        return len(df)
    
    def update_source_metadata(self, source_key: str, source_name: str, 
                               jurisdiction: str, rows: int):
        """Mettre Ã  jour les mÃ©tadonnÃ©es de source"""
        with self.engine.connect() as conn:
            conn.execute(text("""
                INSERT INTO hse_data_sources 
                (source_key, source_name, jurisdiction, last_ingestion, rows_ingested, status)
                VALUES (:key, :name, :jurisdiction, CURRENT_TIMESTAMP, :rows, 'success')
                ON CONFLICT (source_key) DO UPDATE SET
                    last_ingestion = CURRENT_TIMESTAMP,
                    rows_ingested = :rows,
                    status = 'success'
            """), {
                "key": source_key,
                "name": source_name,
                "jurisdiction": jurisdiction,
                "rows": rows
            })
            conn.commit()
    
    def query_summary(self):
        """Obtenir un rÃ©sumÃ© des donnÃ©es"""
        with self.engine.connect() as conn:
            result = conn.execute(text("""
                SELECT 
                    jurisdiction,
                    COUNT(*) as total_incidents,
                    COUNT(DISTINCT year) as years_covered,
                    MIN(year) as first_year,
                    MAX(year) as last_year
                FROM hse_incidents_global
                GROUP BY jurisdiction
                ORDER BY total_incidents DESC
            """))
            
            return pd.DataFrame(result.fetchall(), columns=result.keys())


# Utilisation
if __name__ == "__main__":
    loader = SafetyGraphLoader()
    
    # CrÃ©er les tables
    loader.create_tables()
    
    # Charger les donnÃ©es Gold
    gold_path = "data/gold/hse_incidents_global.parquet"
    loader.load_gold_data(gold_path)
    
    # Afficher le rÃ©sumÃ©
    summary = loader.query_summary()
    print("\nğŸ“Š RÃ©sumÃ© Safety Graph:")
    print(summary)
```

### ExÃ©cution du Chargement

```bash
# Terminal
export POSTGRES_HOST=localhost
export POSTGRES_PORT=5432
export POSTGRES_DB=safety_graph
export POSTGRES_USER=agenticx5
export POSTGRES_PASSWORD=your_password

python postgresql_loader.py
```

---

## â° Automatisation (Scheduling)

### 1. Cron Job (Linux/Mac)

```bash
# Ã‰diter crontab
crontab -e

# Ajouter les jobs
# Ingestion quotidienne OSHA Severe Injuries Ã  6h
0 6 * * * /path/to/venv/bin/python /path/to/hse_data_ingestion.py --source osha_severe_injuries >> /var/log/hse_ingestion.log 2>&1

# Ingestion hebdomadaire complÃ¨te le dimanche Ã  2h
0 2 * * 0 /path/to/venv/bin/python /path/to/hse_data_ingestion.py --all --priority 2 --report >> /var/log/hse_ingestion.log 2>&1

# Fusion mensuelle le 1er Ã  3h
0 3 1 * * /path/to/venv/bin/python /path/to/hse_data_ingestion.py --merge >> /var/log/hse_ingestion.log 2>&1
```

### 2. Windows Task Scheduler

```powershell
# PowerShell - CrÃ©er une tÃ¢che planifiÃ©e
$action = New-ScheduledTaskAction -Execute "python.exe" -Argument "C:\AgenticX5\hse_data_ingestion.py --all --priority 2"
$trigger = New-ScheduledTaskTrigger -Weekly -DaysOfWeek Sunday -At 2am
$settings = New-ScheduledTaskSettingsSet -StartWhenAvailable

Register-ScheduledTask -TaskName "HSE_Ingestion_Weekly" -Action $action -Trigger $trigger -Settings $settings
```

### 3. Script Python avec Schedule

```python
# scheduler.py
"""
Scheduler pour l'ingestion HSE automatique
"""

import schedule
import time
import logging
from datetime import datetime
from hse_data_ingestion import HSEPipelineOrchestrator

logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s | %(levelname)s | %(message)s',
    handlers=[
        logging.FileHandler('hse_scheduler.log'),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)

orchestrator = HSEPipelineOrchestrator(data_dir="./data")


def daily_quick_ingestion():
    """Ingestion quotidienne rapide (sources temps rÃ©el)"""
    logger.info("ğŸ”„ Starting daily quick ingestion...")
    
    quick_sources = [
        "osha_severe_injuries",  # Mis Ã  jour quotidiennement
    ]
    
    for source in quick_sources:
        try:
            result = orchestrator.run_single(source)
            logger.info(f"âœ… {source}: {result['status']}")
        except Exception as e:
            logger.error(f"âŒ {source}: {e}")


def weekly_full_ingestion():
    """Ingestion hebdomadaire complÃ¨te"""
    logger.info("ğŸ”„ Starting weekly full ingestion...")
    
    try:
        results = orchestrator.run_all(priority_threshold=2)
        report = orchestrator.generate_report()
        
        logger.info(f"ğŸ“Š Weekly Report:")
        logger.info(f"   Sources: {report['total_sources']}")
        logger.info(f"   Success: {report['successful']}")
        logger.info(f"   Rows: {report['total_rows_ingested']:,}")
        
    except Exception as e:
        logger.error(f"âŒ Weekly ingestion failed: {e}")


def monthly_consolidation():
    """Consolidation mensuelle"""
    logger.info("ğŸ”„ Starting monthly consolidation...")
    
    try:
        # Fusionner les tables Gold
        merged_path = orchestrator.merge_gold_tables()
        logger.info(f"âœ… Gold tables merged: {merged_path}")
        
        # Charger dans PostgreSQL
        from postgresql_loader import SafetyGraphLoader
        loader = SafetyGraphLoader()
        rows = loader.load_gold_data(str(merged_path), if_exists="replace")
        logger.info(f"âœ… Loaded {rows:,} rows into Safety Graph")
        
    except Exception as e:
        logger.error(f"âŒ Monthly consolidation failed: {e}")


# Configurer le schedule
schedule.every().day.at("06:00").do(daily_quick_ingestion)
schedule.every().sunday.at("02:00").do(weekly_full_ingestion)
schedule.every(1).months.at("03:00").do(monthly_consolidation)  # 1er du mois


if __name__ == "__main__":
    logger.info("ğŸš€ HSE Scheduler started")
    logger.info(f"   Daily: 06:00")
    logger.info(f"   Weekly: Sunday 02:00")
    logger.info(f"   Monthly: 1st 03:00")
    
    while True:
        schedule.run_pending()
        time.sleep(60)  # Check every minute
```

### 4. Apache Airflow DAG

```python
# airflow_dag.py
"""
Apache Airflow DAG pour l'ingestion HSE
"""

from datetime import datetime, timedelta
from airflow import DAG
from airflow.operators.python import PythonOperator
from airflow.operators.bash import BashOperator

default_args = {
    'owner': 'agenticx5',
    'depends_on_past': False,
    'email': ['alerts@genaisafety.com'],
    'email_on_failure': True,
    'email_on_retry': False,
    'retries': 2,
    'retry_delay': timedelta(minutes=5),
}

dag = DAG(
    'hse_data_ingestion',
    default_args=default_args,
    description='Pipeline d\'ingestion HSE multi-sources',
    schedule_interval='0 2 * * 0',  # Dimanche 2h
    start_date=datetime(2026, 1, 1),
    catchup=False,
    tags=['hse', 'safety', 'agenticx5'],
)


def run_ingestion(**context):
    """ExÃ©cuter le pipeline d'ingestion"""
    import sys
    sys.path.append('/opt/airflow/dags/agenticx5')
    
    from hse_data_ingestion import HSEPipelineOrchestrator
    
    orchestrator = HSEPipelineOrchestrator(data_dir="/data/hse")
    results = orchestrator.run_all(priority_threshold=2)
    report = orchestrator.generate_report()
    
    # Push to XCom
    context['ti'].xcom_push(key='ingestion_report', value=report)
    
    return report


def load_to_postgres(**context):
    """Charger dans PostgreSQL"""
    import sys
    sys.path.append('/opt/airflow/dags/agenticx5')
    
    from postgresql_loader import SafetyGraphLoader
    
    loader = SafetyGraphLoader()
    loader.create_tables()
    
    gold_path = "/data/hse/gold/hse_incidents_global.parquet"
    rows = loader.load_gold_data(gold_path)
    
    return rows


# Tasks
t1_ingest = PythonOperator(
    task_id='ingest_hse_data',
    python_callable=run_ingestion,
    dag=dag,
)

t2_merge = BashOperator(
    task_id='merge_gold_tables',
    bash_command='python /opt/airflow/dags/agenticx5/hse_data_ingestion.py --merge --data-dir /data/hse',
    dag=dag,
)

t3_load = PythonOperator(
    task_id='load_to_postgres',
    python_callable=load_to_postgres,
    dag=dag,
)

t4_notify = BashOperator(
    task_id='send_notification',
    bash_command='echo "HSE Ingestion completed at $(date)"',
    dag=dag,
)

# Dependencies
t1_ingest >> t2_merge >> t3_load >> t4_notify
```

---

## ğŸ³ Docker Deployment

### Dockerfile

```dockerfile
# Dockerfile
FROM python:3.11-slim

WORKDIR /app

# Install dependencies
COPY requirements.txt .
RUN pip install --no-cache-dir -r requirements.txt

# Copy application
COPY hse_data_ingestion.py .
COPY postgresql_loader.py .
COPY scheduler.py .

# Create data directories
RUN mkdir -p /data/bronze /data/silver /data/gold

# Environment
ENV DATA_DIR=/data
ENV PYTHONUNBUFFERED=1

# Default command
CMD ["python", "scheduler.py"]
```

### docker-compose.yml

```yaml
version: '3.8'

services:
  hse-ingestion:
    build: .
    container_name: hse-ingestion
    environment:
      - DATA_DIR=/data
      - KAGGLE_USERNAME=${KAGGLE_USERNAME}
      - KAGGLE_KEY=${KAGGLE_KEY}
      - BLS_API_KEY=${BLS_API_KEY}
      - POSTGRES_HOST=postgres
      - POSTGRES_PORT=5432
      - POSTGRES_DB=safety_graph
      - POSTGRES_USER=agenticx5
      - POSTGRES_PASSWORD=${POSTGRES_PASSWORD}
    volumes:
      - hse-data:/data
      - ./logs:/app/logs
    depends_on:
      - postgres
    restart: unless-stopped

  postgres:
    image: postgres:15
    container_name: safety-graph-db
    environment:
      - POSTGRES_DB=safety_graph
      - POSTGRES_USER=agenticx5
      - POSTGRES_PASSWORD=${POSTGRES_PASSWORD}
    volumes:
      - postgres-data:/var/lib/postgresql/data
    ports:
      - "5432:5432"

  pgadmin:
    image: dpage/pgadmin4
    container_name: pgadmin
    environment:
      - PGADMIN_DEFAULT_EMAIL=admin@genaisafety.com
      - PGADMIN_DEFAULT_PASSWORD=${PGADMIN_PASSWORD}
    ports:
      - "5050:80"
    depends_on:
      - postgres

volumes:
  hse-data:
  postgres-data:
```

### Commandes Docker

```bash
# Build et dÃ©marrer
docker-compose up -d --build

# Voir les logs
docker-compose logs -f hse-ingestion

# ExÃ©cuter manuellement une ingestion
docker exec hse-ingestion python hse_data_ingestion.py --all --priority 1

# ArrÃªter
docker-compose down
```

---

## ğŸ“‹ RÃ©sumÃ© des Modes d'ExÃ©cution

| Mode | Environnement | Commande | FrÃ©quence |
|------|---------------|----------|-----------|
| **CLI Local** | Dev | `python hse_data_ingestion.py --source X` | Ad-hoc |
| **Notebook Zerve** | Data Science | Cellules interactives | Ad-hoc |
| **Cron Job** | Linux Server | `crontab -e` | Quotidien/Hebdo |
| **Task Scheduler** | Windows | PowerShell | Quotidien/Hebdo |
| **Python Schedule** | Any | `python scheduler.py` | Continu |
| **Airflow DAG** | Enterprise | Airflow UI | Configurable |
| **Docker** | Container | `docker-compose up` | Continu |

---

## ğŸ”— IntÃ©gration avec Safety Graph

```
Sources HSE          Pipeline               Safety Graph DB
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€       â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€              â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
                         â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”         â”Œâ”€â”€â”€â”€â–¼â”€â”€â”€â”€â”            â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Kaggle  â”‚â”€â”€â”€â”€â”€â”€â”€â”€â–¶â”‚ Bronze  â”‚            â”‚  PostgreSQL     â”‚
â”‚ OSHA    â”‚         â”‚ (Raw)   â”‚            â”‚                 â”‚
â”‚ Eurostatâ”‚         â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜            â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚ ILOSTAT â”‚              â”‚                 â”‚ â”‚hse_incidentsâ”‚ â”‚
â”‚ DARES   â”‚         â”Œâ”€â”€â”€â”€â–¼â”€â”€â”€â”€â”            â”‚ â”‚  _global    â”‚ â”‚
â”‚ CNESST  â”‚â”€â”€â”€â”€â”€â”€â”€â”€â–¶â”‚ Silver  â”‚            â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜         â”‚(Cleaned)â”‚            â”‚                 â”‚
                    â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜            â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
                         â”‚                 â”‚ â”‚hse_data_    â”‚ â”‚
                    â”Œâ”€â”€â”€â”€â–¼â”€â”€â”€â”€â”            â”‚ â”‚  sources    â”‚ â”‚
                    â”‚  Gold   â”‚â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¶â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
                    â”‚(Unified)â”‚            â”‚                 â”‚
                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜            â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                                    â”‚
                                           â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”
                                           â”‚   AgenticX5     â”‚
                                           â”‚   100 Agents    â”‚
                                           â”‚   Dashboard     â”‚
                                           â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

Â© 2026 AgenticX5 â€” GenAISafety / Preventera
