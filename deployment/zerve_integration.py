"""
ZERVE.AI INTEGRATION - SafetyGraph
==================================
Pipeline ML avanc√© pour analyse pr√©dictive SST
Compatible avec Zerve.ai Data Science Platform

Features:
- Connexion PostgreSQL/Neo4j
- Feature engineering automatique
- Export formats Zerve (Parquet, Delta Lake)
- Pipeline MLflow ready
"""

import json
import os
from dataclasses import dataclass, asdict
from typing import Dict, List, Optional, Tuple
from datetime import datetime
import csv

# ============================================
# CONFIGURATION ZERVE.AI
# ============================================

@dataclass
class ZerveConfig:
    """Configuration pour Zerve.ai workspace"""
    workspace_name: str = "SafetyGraph-HSE"
    project_name: str = "AgenticX5-Predictions"
    environment: str = "production"
    
    # Connexions
    postgres_host: str = "localhost"
    postgres_port: int = 5434
    postgres_db: str = "safetygraph"
    postgres_user: str = "safetygraph"
    
    neo4j_uri: str = "bolt://localhost:7688"
    neo4j_user: str = "neo4j"
    
    # MLflow tracking
    mlflow_tracking_uri: str = "http://localhost:5000"
    experiment_name: str = "hse-risk-prediction"
    
    # Feature Store
    feature_store_path: str = "./feature_store"
    
    def to_dict(self) -> Dict:
        return asdict(self)


# ============================================
# FEATURE ENGINEERING POUR SST
# ============================================

class HSEFeatureEngineer:
    """
    Feature Engineering sp√©cialis√© HST/HSE
    G√©n√®re les features pour mod√®les pr√©dictifs
    """
    
    # Features cat√©gorielles
    CATEGORICAL_FEATURES = [
        "sector_scian",
        "event_type",
        "injury_nature",
        "body_part",
        "source_of_injury"
    ]
    
    # Features num√©riques
    NUMERICAL_FEATURES = [
        "year",
        "month",
        "total_cases",
        "days_lost",
        "severity_rate",
        "fatality_rate"
    ]
    
    # Features d√©riv√©es
    DERIVED_FEATURES = [
        "cases_per_1000",
        "days_per_case",
        "seasonal_index",
        "trend_yoy",
        "risk_score"
    ]
    
    def __init__(self):
        self.feature_stats = {}
    
    def compute_derived_features(self, record: Dict) -> Dict:
        """Calcule les features d√©riv√©es pour un enregistrement"""
        total_cases = record.get("total_cases", 0) or record.get("nombre_lesions", 0)
        days_lost = record.get("days_lost", 0) or record.get("jours_perdus", 0)
        deaths = record.get("total_deaths", 0)
        
        # √âviter division par z√©ro
        total_cases = max(1, total_cases)
        
        derived = {
            "days_per_case": round(days_lost / total_cases, 2),
            "fatality_rate": round((deaths / total_cases) * 1000, 4) if deaths else 0,
            "severity_score": self._compute_severity_score(total_cases, days_lost, deaths),
            "risk_category": self._categorize_risk(total_cases, deaths)
        }
        
        return {**record, **derived}
    
    def _compute_severity_score(self, cases: int, days: int, deaths: int) -> float:
        """Score de gravit√© composite (0-100)"""
        # Pond√©ration: d√©c√®s (50%), jours perdus (30%), volume (20%)
        death_score = min(50, deaths * 0.5)
        days_score = min(30, (days / max(1, cases)) * 3)
        volume_score = min(20, cases / 1000)
        return round(death_score + days_score + volume_score, 2)
    
    def _categorize_risk(self, cases: int, deaths: int) -> str:
        """Cat√©gorisation du niveau de risque"""
        if deaths > 50 or cases > 50000:
            return "CRITICAL"
        elif deaths > 20 or cases > 20000:
            return "HIGH"
        elif deaths > 5 or cases > 5000:
            return "MEDIUM"
        else:
            return "LOW"
    
    def generate_feature_vector(self, record: Dict) -> List[float]:
        """G√©n√®re un vecteur de features pour ML"""
        enriched = self.compute_derived_features(record)
        
        vector = [
            float(enriched.get("year", 2023)),
            float(enriched.get("total_cases", 0) or enriched.get("nombre_lesions", 0)),
            float(enriched.get("days_lost", 0) or enriched.get("jours_perdus", 0)),
            float(enriched.get("total_deaths", 0)),
            float(enriched.get("days_per_case", 0)),
            float(enriched.get("fatality_rate", 0)),
            float(enriched.get("severity_score", 0))
        ]
        
        return vector


# ============================================
# PIPELINE DATA POUR ZERVE
# ============================================

class ZerveDataPipeline:
    """
    Pipeline de donn√©es pour Zerve.ai
    Export vers formats compatibles: CSV, Parquet, Delta Lake
    """
    
    def __init__(self, config: ZerveConfig):
        self.config = config
        self.feature_engineer = HSEFeatureEngineer()
        
    def extract_from_postgres(self) -> List[Dict]:
        """
        Extrait les donn√©es harmonis√©es depuis PostgreSQL
        Note: En production, utiliser psycopg2 ou sqlalchemy
        """
        # Simulation des donn√©es extraites (en prod: vraie connexion)
        # Cette structure correspond √† nos tables
        return [
            # CNESST Data
            {"source": "CNESST", "year": 2023, "sector": "221122", "total_cases": 748, "days_lost": 7480, "total_deaths": 3},
            {"source": "CNESST", "year": 2023, "sector": "23", "total_cases": 7518, "days_lost": 52626, "total_deaths": 12},
            {"source": "CNESST", "year": 2023, "sector": "62", "total_cases": 21346, "days_lost": 85384, "total_deaths": 5},
            {"source": "CNESST", "year": 2023, "sector": "31-33", "total_cases": 12252, "days_lost": 73512, "total_deaths": 8},
            # OSHA Data
            {"source": "OSHA", "year": 2023, "sector": "2211", "total_cases": 1850, "days_lost": 18500, "total_deaths": 82},
            {"source": "OSHA", "year": 2023, "sector": "23", "total_cases": 46200, "days_lost": 462000, "total_deaths": 423},
            {"source": "OSHA", "year": 2023, "sector": "62", "total_cases": 124000, "days_lost": 744000, "total_deaths": 23},
            {"source": "OSHA", "year": 2023, "sector": "31-33", "total_cases": 34500, "days_lost": 241500, "total_deaths": 67},
        ]
    
    def transform_for_ml(self, data: List[Dict]) -> List[Dict]:
        """Transforme les donn√©es pour ML avec feature engineering"""
        transformed = []
        for record in data:
            enriched = self.feature_engineer.compute_derived_features(record)
            transformed.append(enriched)
        return transformed
    
    def export_to_csv(self, data: List[Dict], filename: str) -> str:
        """Export vers CSV pour Zerve"""
        if not data:
            return ""
        
        filepath = f"{self.config.feature_store_path}/{filename}"
        os.makedirs(self.config.feature_store_path, exist_ok=True)
        
        with open(filepath, 'w', newline='', encoding='utf-8') as f:
            writer = csv.DictWriter(f, fieldnames=data[0].keys())
            writer.writeheader()
            writer.writerows(data)
        
        return filepath
    
    def generate_zerve_notebook_config(self) -> Dict:
        """G√©n√®re la configuration pour notebook Zerve"""
        return {
            "workspace": self.config.workspace_name,
            "project": self.config.project_name,
            "environment": {
                "python_version": "3.10",
                "packages": [
                    "pandas>=2.0",
                    "scikit-learn>=1.3",
                    "xgboost>=2.0",
                    "lightgbm>=4.0",
                    "mlflow>=2.8",
                    "shap>=0.43",
                    "plotly>=5.18"
                ]
            },
            "data_sources": [
                {
                    "name": "safetygraph_postgres",
                    "type": "postgresql",
                    "host": self.config.postgres_host,
                    "port": self.config.postgres_port,
                    "database": self.config.postgres_db
                },
                {
                    "name": "safetygraph_neo4j",
                    "type": "neo4j",
                    "uri": self.config.neo4j_uri
                }
            ],
            "experiments": [
                {
                    "name": "risk_prediction_xgboost",
                    "model_type": "XGBClassifier",
                    "target": "risk_category",
                    "features": HSEFeatureEngineer.NUMERICAL_FEATURES
                },
                {
                    "name": "severity_regression",
                    "model_type": "XGBRegressor",
                    "target": "severity_score",
                    "features": HSEFeatureEngineer.NUMERICAL_FEATURES
                },
                {
                    "name": "incident_forecasting",
                    "model_type": "Prophet",
                    "target": "total_cases",
                    "time_column": "year"
                }
            ]
        }


# ============================================
# ZERVE NOTEBOOK TEMPLATE
# ============================================

ZERVE_NOTEBOOK_TEMPLATE = '''
# SafetyGraph - HSE Risk Prediction Pipeline
# Zerve.ai Workspace: {workspace}

# %% [markdown]
# ## 1. Setup & Data Loading

# %%
import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder
from xgboost import XGBClassifier, XGBRegressor
import mlflow
import shap

# Configuration MLflow
mlflow.set_tracking_uri("{mlflow_uri}")
mlflow.set_experiment("{experiment}")

# %% [markdown]
# ## 2. Load Data from SafetyGraph

# %%
# Connexion PostgreSQL
import psycopg2

conn = psycopg2.connect(
    host="{pg_host}",
    port={pg_port},
    database="{pg_db}",
    user="{pg_user}",
    password="SafetyGraph2026!"
)

# Charger donn√©es CNESST + OSHA harmonis√©es
df_cnesst = pd.read_sql("SELECT * FROM cnesst_lesions", conn)
df_osha = pd.read_sql("SELECT * FROM osha_injuries", conn)

print(f"CNESST: {{len(df_cnesst)}} records")
print(f"OSHA: {{len(df_osha)}} records")

# %% [markdown]
# ## 3. Feature Engineering

# %%
# Calculer features d√©riv√©es
df_cnesst['days_per_case'] = df_cnesst['jours_perdus'] / df_cnesst['nombre_lesions'].clip(lower=1)
df_cnesst['source'] = 'CNESST'

df_osha['days_per_case'] = df_osha['days_away'] / df_osha['total_cases'].clip(lower=1)
df_osha['fatality_rate'] = df_osha['total_deaths'] / df_osha['total_cases'].clip(lower=1) * 1000
df_osha['source'] = 'OSHA'

# %% [markdown]
# ## 4. Model Training - Risk Classification

# %%
with mlflow.start_run(run_name="risk_classification_v1"):
    # Pr√©parer features
    X = df_osha[['total_cases', 'days_away', 'total_deaths', 'days_per_case']].fillna(0)
    y = (df_osha['total_deaths'] > 50).astype(int)  # High risk si >50 d√©c√®s
    
    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
    
    # Entra√Æner XGBoost
    model = XGBClassifier(n_estimators=100, max_depth=5, random_state=42)
    model.fit(X_train, y_train)
    
    # √âvaluation
    accuracy = model.score(X_test, y_test)
    mlflow.log_metric("accuracy", accuracy)
    mlflow.sklearn.log_model(model, "risk_classifier")
    
    print(f"Accuracy: {{accuracy:.2%}}")

# %% [markdown]
# ## 5. SHAP Explainability

# %%
explainer = shap.TreeExplainer(model)
shap_values = explainer.shap_values(X_test)
shap.summary_plot(shap_values, X_test, feature_names=X.columns.tolist())

# %% [markdown]
# ## 6. Save to Feature Store

# %%
# Export pour r√©utilisation
df_cnesst.to_parquet("feature_store/cnesst_features.parquet")
df_osha.to_parquet("feature_store/osha_features.parquet")
print("‚úÖ Features export√©es vers Feature Store")
'''


# ============================================
# MAIN EXECUTION
# ============================================

if __name__ == "__main__":
    print("=" * 60)
    print("ZERVE.AI INTEGRATION - SafetyGraph")
    print("=" * 60)
    
    # Configuration
    config = ZerveConfig()
    pipeline = ZerveDataPipeline(config)
    
    # 1. Extraire donn√©es
    print("\nüìä Extraction des donn√©es...")
    raw_data = pipeline.extract_from_postgres()
    print(f"   {len(raw_data)} enregistrements extraits")
    
    # 2. Transformer avec feature engineering
    print("\n‚öôÔ∏è Feature Engineering...")
    ml_data = pipeline.transform_for_ml(raw_data)
    
    for record in ml_data[:3]:
        print(f"   {record['source']} | Secteur {record['sector']}: "
              f"Score={record['severity_score']}, Risque={record['risk_category']}")
    
    # 3. Export CSV
    print("\nüíæ Export vers Feature Store...")
    os.makedirs("./feature_store", exist_ok=True)
    csv_path = pipeline.export_to_csv(ml_data, "hse_features_harmonized.csv")
    print(f"   Fichier: {csv_path}")
    
    # 4. G√©n√©rer config Zerve
    print("\nüìù Configuration Zerve.ai g√©n√©r√©e...")
    zerve_config = pipeline.generate_zerve_notebook_config()
    
    with open("./feature_store/zerve_config.json", "w") as f:
        json.dump(zerve_config, f, indent=2)
    print("   Fichier: ./feature_store/zerve_config.json")
    
    # 5. G√©n√©rer notebook template
    notebook_content = ZERVE_NOTEBOOK_TEMPLATE.format(
        workspace=config.workspace_name,
        mlflow_uri=config.mlflow_tracking_uri,
        experiment=config.experiment_name,
        pg_host=config.postgres_host,
        pg_port=config.postgres_port,
        pg_db=config.postgres_db,
        pg_user=config.postgres_user
    )
    
    with open("./feature_store/safetygraph_zerve_notebook.py", "w") as f:
        f.write(notebook_content)
    print("   Fichier: ./feature_store/safetygraph_zerve_notebook.py")
    
    # R√©sum√©
    print("\n" + "=" * 60)
    print("‚úÖ INT√âGRATION ZERVE.AI COMPL√àTE")
    print("=" * 60)
    print(f"""
üìÅ Fichiers g√©n√©r√©s:
   ‚Ä¢ feature_store/hse_features_harmonized.csv
   ‚Ä¢ feature_store/zerve_config.json
   ‚Ä¢ feature_store/safetygraph_zerve_notebook.py

üîó Pour utiliser dans Zerve.ai:
   1. Cr√©er workspace: {config.workspace_name}
   2. Importer zerve_config.json
   3. Ex√©cuter safetygraph_zerve_notebook.py

üìä Exp√©riences ML configur√©es:
   ‚Ä¢ risk_prediction_xgboost (Classification)
   ‚Ä¢ severity_regression (R√©gression)
   ‚Ä¢ incident_forecasting (Time Series)
""")
