# üîß Guide d'Int√©gration : HSE Ingestion Package ‚Üí SafetyGraph

## üìã Vue d'Ensemble

Ce guide d√©taille l'int√©gration compl√®te du `hse_ingestion_package` dans l'√©cosyst√®me SafetyGraph, permettant d'alimenter automatiquement le Knowledge Graph Neo4j et les 110+ agents LangGraph.

---

## üèóÔ∏è Architecture Cible

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                         HSE INGESTION PACKAGE                           ‚îÇ
‚îÇ   github.com/Preventera/hse_ingestion_package                          ‚îÇ
‚îÇ                                                                         ‚îÇ
‚îÇ   ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê          ‚îÇ
‚îÇ   ‚îÇ  OSHA   ‚îÇ ‚îÇEurostat ‚îÇ ‚îÇ ILOSTAT ‚îÇ ‚îÇ  CNESST ‚îÇ ‚îÇ  BLS    ‚îÇ  ...     ‚îÇ
‚îÇ   ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îò ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îò ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îò ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îò ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îò          ‚îÇ
‚îÇ        ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò               ‚îÇ
‚îÇ                              ‚îÇ                                          ‚îÇ
‚îÇ                    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê                               ‚îÇ
‚îÇ                    ‚îÇ hse_data_ingestion‚îÇ                               ‚îÇ
‚îÇ                    ‚îÇ   (normalisation) ‚îÇ                               ‚îÇ
‚îÇ                    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò                               ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                               ‚îÇ
              ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
              ‚îÇ                ‚îÇ                ‚îÇ
              ‚ñº                ‚ñº                ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ   PostgreSQL    ‚îÇ  ‚îÇ    Neo4j KG     ‚îÇ  ‚îÇ   ChromaDB      ‚îÇ
‚îÇ  (structured)   ‚îÇ  ‚îÇ  (ontologies)   ‚îÇ  ‚îÇ  (embeddings)   ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
         ‚îÇ                    ‚îÇ                    ‚îÇ
         ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                              ‚îÇ
                    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
                    ‚îÇ   110+ Agents     ‚îÇ
                    ‚îÇ    LangGraph      ‚îÇ
                    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

---

## üìÅ Structure de Fichiers Recommand√©e

```bash
SafeGraph/
‚îú‚îÄ‚îÄ safetygraph-data/                    # D√©p√¥t donn√©es (existant ou nouveau)
‚îÇ   ‚îî‚îÄ‚îÄ etl/
‚îÇ       ‚îî‚îÄ‚îÄ hse_ingestion/               # ‚Üê NOUVEAU MODULE
‚îÇ           ‚îú‚îÄ‚îÄ __init__.py
‚îÇ           ‚îú‚îÄ‚îÄ config/
‚îÇ           ‚îÇ   ‚îú‚îÄ‚îÄ sources.yaml         # Configuration sources
‚îÇ           ‚îÇ   ‚îî‚îÄ‚îÄ credentials.env      # Cl√©s API (gitignored)
‚îÇ           ‚îú‚îÄ‚îÄ connectors/
‚îÇ           ‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
‚îÇ           ‚îÇ   ‚îú‚îÄ‚îÄ base_connector.py    # Classe abstraite
‚îÇ           ‚îÇ   ‚îú‚îÄ‚îÄ osha_connector.py    
‚îÇ           ‚îÇ   ‚îú‚îÄ‚îÄ eurostat_connector.py
‚îÇ           ‚îÇ   ‚îú‚îÄ‚îÄ ilostat_connector.py
‚îÇ           ‚îÇ   ‚îú‚îÄ‚îÄ cnesst_connector.py
‚îÇ           ‚îÇ   ‚îî‚îÄ‚îÄ kaggle_connector.py
‚îÇ           ‚îú‚îÄ‚îÄ loaders/
‚îÇ           ‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
‚îÇ           ‚îÇ   ‚îú‚îÄ‚îÄ postgresql_loader.py # Existant (adapt√©)
‚îÇ           ‚îÇ   ‚îú‚îÄ‚îÄ neo4j_kg_loader.py   # ‚Üê NOUVEAU
‚îÇ           ‚îÇ   ‚îî‚îÄ‚îÄ chroma_loader.py     # ‚Üê NOUVEAU
‚îÇ           ‚îú‚îÄ‚îÄ transformers/
‚îÇ           ‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
‚îÇ           ‚îÇ   ‚îú‚îÄ‚îÄ normalizer.py        # Normalisation multi-juridictionnelle
‚îÇ           ‚îÇ   ‚îî‚îÄ‚îÄ triplet_generator.py # G√©n√©ration triplets RDF
‚îÇ           ‚îú‚îÄ‚îÄ orchestrator.py          # Orchestrateur principal
‚îÇ           ‚îî‚îÄ‚îÄ cli.py                   # Interface ligne de commande
‚îÇ
‚îú‚îÄ‚îÄ safetygraph-kg/                      # Module Knowledge Graph (existant)
‚îÇ   ‚îî‚îÄ‚îÄ backend/
‚îÇ       ‚îî‚îÄ‚îÄ etl/                         # Lien symbolique vers hse_ingestion
‚îÇ
‚îî‚îÄ‚îÄ config/
    ‚îî‚îÄ‚îÄ airflow/
        ‚îî‚îÄ‚îÄ dags/
            ‚îî‚îÄ‚îÄ hse_ingestion_dag.py     # DAG Airflow pour orchestration
```

---

## üöÄ √âtapes d'Int√©gration

### √âtape 1 : Cloner et Positionner le Package

```powershell
# Dans votre r√©pertoire SafeGraph principal
cd C:\Users\Mario\Documents\PROJECTS_NEW\SafeGraph

# Cloner le package d'ingestion
git clone https://github.com/Preventera/hse_ingestion_package.git safetygraph-data/etl/hse_ingestion_source

# Cr√©er la structure d'int√©gration
mkdir -p safetygraph-data/etl/hse_ingestion/connectors
mkdir -p safetygraph-data/etl/hse_ingestion/loaders
mkdir -p safetygraph-data/etl/hse_ingestion/transformers
mkdir -p safetygraph-data/etl/hse_ingestion/config
```

### √âtape 2 : Cr√©er le Fichier de Configuration

Cr√©er `safetygraph-data/etl/hse_ingestion/config/sources.yaml`:

```yaml
# Configuration Sources HSE - SafetyGraph Integration
# =================================================

global:
  batch_size: 10000
  retry_attempts: 3
  timeout_seconds: 300
  output_format: "parquet"  # parquet, csv, json

sources:
  # ============ USA ============
  osha_inspections:
    enabled: true
    priority: 1
    type: api
    url: "https://enforcedata.dol.gov/api/osha/inspection"
    schedule: "0 2 * * *"  # Daily 2AM
    fields_mapping:
      date: "open_date"
      sector: "sic_code"
      severity: "total_penalty"
      
  osha_severe_injuries:
    enabled: true
    priority: 1
    type: api
    url: "https://enforcedata.dol.gov/api/osha/severe_injury"
    schedule: "0 3 * * *"
    
  kaggle_osha_injuries:
    enabled: true
    priority: 2
    type: kaggle
    dataset: "ruqaiya/osha-accident-and-injury-data"
    schedule: "0 4 * * 0"  # Weekly Sunday
    
  # ============ EUROPE ============
  eurostat_esaw:
    enabled: true
    priority: 1
    type: sdmx
    url: "https://ec.europa.eu/eurostat/api/dissemination"
    datasets:
      - "hsw_mi01"  # Accidents at work
      - "hsw_mi02"  # Fatal accidents
    countries: ["FR", "DE", "IT", "ES", "BE", "NL"]
    schedule: "0 5 * * 1"  # Weekly Monday
    
  # ============ INTERNATIONAL ============
  ilostat:
    enabled: true
    priority: 2
    type: api
    url: "https://ilostat.ilo.org/data/sdmx"
    indicators:
      - "INJ_FATL_SEX_ECO_NB_A"
      - "INJ_NFTL_SEX_ECO_NB_A"
    schedule: "0 6 1 * *"  # Monthly
    
  # ============ CANADA/QU√âBEC ============
  cnesst_opendata:
    enabled: true
    priority: 1
    type: csv
    url: "https://www.donneesquebec.ca/recherche/dataset/cnesst"
    datasets:
      - "lesions-professionnelles"
      - "maladies-professionnelles"
    schedule: "0 1 * * *"  # Daily 1AM (prioritaire)
    
# Mapping SCIAN pour uniformisation
scian_mapping:
  construction: "23"
  manufacturing: "31-33"
  mining: "21"
  healthcare: "62"
  transportation: "48-49"
  agriculture: "11"
  
# Destinations SafetyGraph
destinations:
  postgresql:
    enabled: true
    schema: "hse_raw"
    
  neo4j:
    enabled: true
    database: "safetygraph"
    
  chromadb:
    enabled: true
    collection: "hse_incidents"
```

### √âtape 3 : Cr√©er le Loader Neo4j pour Knowledge Graph

Cr√©er `safetygraph-data/etl/hse_ingestion/loaders/neo4j_kg_loader.py`:

```python
"""
Neo4j Knowledge Graph Loader pour SafetyGraph
Transforme les donn√©es HSE en triplets et les charge dans Neo4j
"""

import os
import logging
from typing import Dict, List, Any, Optional
from datetime import datetime
from neo4j import GraphDatabase
import pandas as pd

logger = logging.getLogger(__name__)

class Neo4jKGLoader:
    """
    Loader pour charger les donn√©es HSE dans le Knowledge Graph Neo4j SafetyGraph.
    Compatible avec l'ontologie existante et les 110+ agents.
    """
    
    def __init__(self, uri: str = None, user: str = None, password: str = None):
        self.uri = uri or os.getenv("NEO4J_URI", "bolt://localhost:7687")
        self.user = user or os.getenv("NEO4J_USER", "neo4j")
        self.password = password or os.getenv("NEO4J_PASSWORD")
        self.driver = None
        
    def connect(self):
        """√âtablir la connexion Neo4j"""
        try:
            self.driver = GraphDatabase.driver(
                self.uri, 
                auth=(self.user, self.password)
            )
            self.driver.verify_connectivity()
            logger.info(f"‚úÖ Connexion Neo4j √©tablie: {self.uri}")
        except Exception as e:
            logger.error(f"‚ùå Erreur connexion Neo4j: {e}")
            raise
            
    def close(self):
        """Fermer la connexion"""
        if self.driver:
            self.driver.close()
            
    def setup_constraints(self):
        """Cr√©er les contraintes et index pour SafetyGraph"""
        constraints = [
            # Contraintes d'unicit√©
            "CREATE CONSTRAINT incident_uri IF NOT EXISTS FOR (i:Incident) REQUIRE i.uri IS UNIQUE",
            "CREATE CONSTRAINT sector_code IF NOT EXISTS FOR (s:Sector) REQUIRE s.code IS UNIQUE",
            "CREATE CONSTRAINT lesion_type IF NOT EXISTS FOR (l:LesionType) REQUIRE l.code IS UNIQUE",
            "CREATE CONSTRAINT jurisdiction IF NOT EXISTS FOR (j:Jurisdiction) REQUIRE j.code IS UNIQUE",
            "CREATE CONSTRAINT agent_id IF NOT EXISTS FOR (a:SafetyAgent) REQUIRE a.id IS UNIQUE",
            
            # Index pour performance
            "CREATE INDEX incident_date IF NOT EXISTS FOR (i:Incident) ON (i.date)",
            "CREATE INDEX incident_severity IF NOT EXISTS FOR (i:Incident) ON (i.severity)",
            "CREATE INDEX sector_scian IF NOT EXISTS FOR (s:Sector) ON (s.scian_code)",
        ]
        
        with self.driver.session() as session:
            for constraint in constraints:
                try:
                    session.run(constraint)
                    logger.info(f"‚úÖ Contrainte cr√©√©e: {constraint.split()[2]}")
                except Exception as e:
                    logger.debug(f"Contrainte existante: {constraint.split()[2]}")
                    
    def load_incidents(self, df: pd.DataFrame, source: str, batch_size: int = 1000):
        """
        Charger les incidents dans le Knowledge Graph.
        
        Args:
            df: DataFrame avec colonnes normalis√©es
            source: Source des donn√©es (osha, cnesst, eurostat, etc.)
            batch_size: Taille des lots pour insertion
        """
        logger.info(f"üì• Chargement {len(df)} incidents depuis {source}")
        
        # Normaliser les colonnes
        required_cols = ['incident_id', 'date', 'sector_code', 'injury_type', 
                        'severity', 'jurisdiction']
        
        for col in required_cols:
            if col not in df.columns:
                logger.warning(f"‚ö†Ô∏è Colonne manquante: {col}")
                
        incidents_loaded = 0
        
        with self.driver.session() as session:
            for i in range(0, len(df), batch_size):
                batch = df.iloc[i:i+batch_size].to_dict('records')
                
                query = """
                UNWIND $incidents AS inc
                
                // Cr√©er ou matcher la juridiction
                MERGE (j:Jurisdiction {code: inc.jurisdiction})
                ON CREATE SET j.name = inc.jurisdiction_name,
                              j.created_at = datetime()
                
                // Cr√©er ou matcher le secteur
                MERGE (s:Sector {code: inc.sector_code})
                ON CREATE SET s.name = inc.sector_name,
                              s.scian_code = inc.scian_code,
                              s.created_at = datetime()
                
                // Cr√©er ou matcher le type de l√©sion
                MERGE (lt:LesionType {code: inc.injury_type})
                ON CREATE SET lt.name = inc.injury_name,
                              lt.severity_class = inc.severity_class,
                              lt.created_at = datetime()
                
                // Cr√©er l'incident
                MERGE (i:Incident {uri: inc.incident_uri})
                ON CREATE SET i.date = date(inc.date),
                              i.severity = inc.severity,
                              i.description = inc.description,
                              i.source = $source,
                              i.created_at = datetime()
                ON MATCH SET i.updated_at = datetime()
                
                // Cr√©er les relations
                MERGE (i)-[:OCCURRED_IN]->(j)
                MERGE (i)-[:IN_SECTOR]->(s)
                MERGE (i)-[:HAS_INJURY]->(lt)
                
                // Assigner aux agents SafetyGraph pertinents
                WITH i, s
                MATCH (a:SafetyAgent)
                WHERE a.sector_coverage CONTAINS s.scian_code
                MERGE (a)-[:MONITORS]->(i)
                
                RETURN count(i) as loaded
                """
                
                result = session.run(query, incidents=batch, source=source)
                count = result.single()['loaded']
                incidents_loaded += count
                
                logger.info(f"  Lot {i//batch_size + 1}: {count} incidents charg√©s")
                
        logger.info(f"‚úÖ Total charg√©: {incidents_loaded} incidents")
        return incidents_loaded
        
    def load_triplets_rdf(self, triplets: List[Dict[str, str]]):
        """
        Charger des triplets RDF directement dans le KG.
        
        Args:
            triplets: Liste de dicts avec 'subject', 'predicate', 'object'
        """
        logger.info(f"üì• Chargement {len(triplets)} triplets RDF")
        
        query = """
        UNWIND $triplets AS t
        
        MERGE (s:Entity {uri: t.subject})
        MERGE (o:Entity {uri: t.object})
        
        WITH s, o, t
        CALL apoc.merge.relationship(s, t.predicate, {}, {created_at: datetime()}, o) 
        YIELD rel
        
        RETURN count(rel) as created
        """
        
        with self.driver.session() as session:
            result = session.run(query, triplets=triplets)
            count = result.single()['created']
            logger.info(f"‚úÖ {count} relations cr√©√©es")
            return count
            
    def link_to_agents(self, sector_code: str):
        """
        Lier les nouveaux incidents aux agents SafetyGraph appropri√©s.
        
        Args:
            sector_code: Code SCIAN du secteur
        """
        query = """
        // Trouver les incidents non assign√©s dans ce secteur
        MATCH (i:Incident)-[:IN_SECTOR]->(s:Sector {scian_code: $sector_code})
        WHERE NOT (i)<-[:MONITORS]-(:SafetyAgent)
        
        // Trouver les agents comp√©tents
        MATCH (a:SafetyAgent)
        WHERE a.sector_coverage CONTAINS $sector_code
           OR a.type = 'generalist'
        
        // Cr√©er les liens
        MERGE (a)-[:MONITORS]->(i)
        
        RETURN count(DISTINCT i) as incidents_linked,
               collect(DISTINCT a.id) as agents_assigned
        """
        
        with self.driver.session() as session:
            result = session.run(query, sector_code=sector_code)
            data = result.single()
            logger.info(f"‚úÖ {data['incidents_linked']} incidents li√©s aux agents: {data['agents_assigned']}")
            
    def get_stats(self) -> Dict[str, int]:
        """Obtenir les statistiques du KG"""
        query = """
        MATCH (i:Incident) WITH count(i) as incidents
        MATCH (s:Sector) WITH incidents, count(s) as sectors
        MATCH (j:Jurisdiction) WITH incidents, sectors, count(j) as jurisdictions
        MATCH (a:SafetyAgent) WITH incidents, sectors, jurisdictions, count(a) as agents
        MATCH ()-[r]->() WITH incidents, sectors, jurisdictions, agents, count(r) as relations
        
        RETURN {
            incidents: incidents,
            sectors: sectors,
            jurisdictions: jurisdictions,
            agents: agents,
            relations: relations
        } as stats
        """
        
        with self.driver.session() as session:
            result = session.run(query)
            return result.single()['stats']


# ============ EXEMPLE D'UTILISATION ============

if __name__ == "__main__":
    import pandas as pd
    
    # Configuration
    loader = Neo4jKGLoader()
    loader.connect()
    loader.setup_constraints()
    
    # Exemple de donn√©es normalis√©es
    sample_data = pd.DataFrame([
        {
            'incident_uri': 'safetygraph:incident:osha:2024:001',
            'incident_id': 'OSHA-2024-001',
            'date': '2024-01-15',
            'sector_code': '23',
            'sector_name': 'Construction',
            'scian_code': '23',
            'injury_type': 'FALL',
            'injury_name': 'Chute de hauteur',
            'severity': 'HIGH',
            'severity_class': 3,
            'jurisdiction': 'USA-OSHA',
            'jurisdiction_name': 'United States - OSHA',
            'description': 'Fall from scaffolding at construction site'
        },
        {
            'incident_uri': 'safetygraph:incident:cnesst:2024:001',
            'incident_id': 'CNESST-2024-001',
            'date': '2024-01-16',
            'sector_code': '23',
            'sector_name': 'Construction',
            'scian_code': '23',
            'injury_type': 'TMS',
            'injury_name': 'Trouble musculosquelettique',
            'severity': 'MEDIUM',
            'severity_class': 2,
            'jurisdiction': 'CA-QC-CNESST',
            'jurisdiction_name': 'Qu√©bec - CNESST',
            'description': 'Douleur lombaire suite √† levage r√©p√©titif'
        }
    ])
    
    # Charger les donn√©es
    loader.load_incidents(sample_data, source='test')
    
    # Obtenir les stats
    stats = loader.get_stats()
    print(f"\nüìä Statistiques KG: {stats}")
    
    loader.close()
```

### √âtape 4 : Cr√©er l'Orchestrateur Principal

Cr√©er `safetygraph-data/etl/hse_ingestion/orchestrator.py`:

```python
"""
Orchestrateur Principal HSE Ingestion pour SafetyGraph
Coordonne l'ingestion multi-sources vers les bases de donn√©es SafetyGraph
"""

import os
import yaml
import logging
from datetime import datetime
from typing import Dict, List, Optional
import pandas as pd
from pathlib import Path

# Import des loaders
from loaders.neo4j_kg_loader import Neo4jKGLoader
from loaders.postgresql_loader import PostgreSQLLoader

logger = logging.getLogger(__name__)

class HSEIngestionOrchestrator:
    """
    Orchestrateur pour l'ingestion des donn√©es HSE dans SafetyGraph.
    Coordonne les connecteurs, transformations et chargements.
    """
    
    def __init__(self, config_path: str = None):
        self.config_path = config_path or "config/sources.yaml"
        self.config = self._load_config()
        self.loaders = {}
        self.stats = {
            'start_time': None,
            'end_time': None,
            'sources_processed': 0,
            'records_ingested': 0,
            'errors': []
        }
        
    def _load_config(self) -> Dict:
        """Charger la configuration des sources"""
        with open(self.config_path, 'r', encoding='utf-8') as f:
            return yaml.safe_load(f)
            
    def initialize_loaders(self):
        """Initialiser les connexions aux bases de donn√©es"""
        destinations = self.config.get('destinations', {})
        
        if destinations.get('neo4j', {}).get('enabled'):
            self.loaders['neo4j'] = Neo4jKGLoader()
            self.loaders['neo4j'].connect()
            self.loaders['neo4j'].setup_constraints()
            logger.info("‚úÖ Loader Neo4j initialis√©")
            
        if destinations.get('postgresql', {}).get('enabled'):
            self.loaders['postgresql'] = PostgreSQLLoader()
            self.loaders['postgresql'].connect()
            logger.info("‚úÖ Loader PostgreSQL initialis√©")
            
    def close_loaders(self):
        """Fermer les connexions"""
        for name, loader in self.loaders.items():
            try:
                loader.close()
                logger.info(f"‚úÖ Loader {name} ferm√©")
            except Exception as e:
                logger.error(f"‚ùå Erreur fermeture {name}: {e}")
                
    def normalize_data(self, df: pd.DataFrame, source: str) -> pd.DataFrame:
        """
        Normaliser les donn√©es selon les standards SafetyGraph.
        
        Args:
            df: DataFrame brut de la source
            source: Nom de la source
            
        Returns:
            DataFrame normalis√© avec colonnes standards
        """
        mapping = self.config['sources'].get(source, {}).get('fields_mapping', {})
        scian_mapping = self.config.get('scian_mapping', {})
        
        # Renommer les colonnes selon le mapping
        if mapping:
            df = df.rename(columns=mapping)
            
        # Ajouter l'URI unique SafetyGraph
        df['incident_uri'] = df.apply(
            lambda row: f"safetygraph:incident:{source}:{row.get('date', 'unknown')}:{row.name}",
            axis=1
        )
        
        # Normaliser les codes secteur vers SCIAN
        if 'sector' in df.columns:
            df['scian_code'] = df['sector'].map(
                lambda x: scian_mapping.get(str(x).lower(), x)
            )
            
        # Normaliser la s√©v√©rit√©
        severity_map = {
            'fatal': 'CRITICAL', 'fatality': 'CRITICAL', 'death': 'CRITICAL',
            'hospitalization': 'HIGH', 'serious': 'HIGH', 'severe': 'HIGH',
            'moderate': 'MEDIUM', 'minor': 'LOW', 'first_aid': 'LOW'
        }
        if 'severity' in df.columns:
            df['severity_normalized'] = df['severity'].str.lower().map(
                lambda x: severity_map.get(x, 'MEDIUM')
            )
            
        # Ajouter m√©tadonn√©es
        df['source'] = source
        df['ingested_at'] = datetime.utcnow().isoformat()
        
        return df
        
    def ingest_source(self, source_name: str) -> int:
        """
        Ing√©rer une source sp√©cifique.
        
        Args:
            source_name: Nom de la source (ex: 'osha_inspections')
            
        Returns:
            Nombre d'enregistrements ing√©r√©s
        """
        source_config = self.config['sources'].get(source_name)
        if not source_config:
            logger.error(f"‚ùå Source inconnue: {source_name}")
            return 0
            
        if not source_config.get('enabled', True):
            logger.info(f"‚è≠Ô∏è Source d√©sactiv√©e: {source_name}")
            return 0
            
        logger.info(f"üîÑ Ingestion de {source_name}...")
        
        try:
            # 1. Extraction (via connecteur appropri√©)
            connector_type = source_config.get('type')
            df = self._extract_data(source_name, source_config)
            
            if df is None or df.empty:
                logger.warning(f"‚ö†Ô∏è Aucune donn√©e extraite de {source_name}")
                return 0
                
            logger.info(f"  üì¶ {len(df)} enregistrements extraits")
            
            # 2. Transformation/Normalisation
            df_normalized = self.normalize_data(df, source_name)
            
            # 3. Chargement vers Neo4j KG
            if 'neo4j' in self.loaders:
                count = self.loaders['neo4j'].load_incidents(
                    df_normalized, 
                    source=source_name
                )
                logger.info(f"  üìä {count} enregistrements charg√©s dans Neo4j")
                
            # 4. Chargement vers PostgreSQL (backup/analytics)
            if 'postgresql' in self.loaders:
                self.loaders['postgresql'].load_dataframe(
                    df_normalized,
                    table=f"hse_raw.{source_name}",
                    if_exists='append'
                )
                
            self.stats['records_ingested'] += len(df_normalized)
            return len(df_normalized)
            
        except Exception as e:
            logger.error(f"‚ùå Erreur ingestion {source_name}: {e}")
            self.stats['errors'].append({
                'source': source_name,
                'error': str(e),
                'timestamp': datetime.utcnow().isoformat()
            })
            return 0
            
    def _extract_data(self, source_name: str, config: Dict) -> Optional[pd.DataFrame]:
        """Extraire les donn√©es selon le type de source"""
        connector_type = config.get('type')
        
        if connector_type == 'api':
            from connectors.api_connector import APIConnector
            connector = APIConnector(config['url'])
            return connector.fetch()
            
        elif connector_type == 'kaggle':
            from connectors.kaggle_connector import KaggleConnector
            connector = KaggleConnector(config['dataset'])
            return connector.fetch()
            
        elif connector_type == 'csv':
            from connectors.csv_connector import CSVConnector
            connector = CSVConnector(config['url'])
            return connector.fetch()
            
        elif connector_type == 'sdmx':
            from connectors.eurostat_connector import EurostatConnector
            connector = EurostatConnector(config['url'])
            return connector.fetch(config.get('datasets', []))
            
        else:
            logger.error(f"Type de connecteur inconnu: {connector_type}")
            return None
            
    def run_all(self, priority: int = None):
        """
        Ex√©cuter l'ingestion de toutes les sources.
        
        Args:
            priority: Si sp√©cifi√©, n'ing√®re que les sources de cette priorit√©
        """
        self.stats['start_time'] = datetime.utcnow().isoformat()
        logger.info("üöÄ D√©marrage ingestion HSE compl√®te")
        
        self.initialize_loaders()
        
        sources = self.config.get('sources', {})
        for source_name, source_config in sources.items():
            if priority and source_config.get('priority') != priority:
                continue
                
            self.ingest_source(source_name)
            self.stats['sources_processed'] += 1
            
        self.close_loaders()
        
        self.stats['end_time'] = datetime.utcnow().isoformat()
        self._print_summary()
        
    def _print_summary(self):
        """Afficher le r√©sum√© de l'ingestion"""
        print("\n" + "="*60)
        print("üìä R√âSUM√â INGESTION HSE ‚Üí SAFETYGRAPH")
        print("="*60)
        print(f"‚è±Ô∏è  D√©but: {self.stats['start_time']}")
        print(f"‚è±Ô∏è  Fin: {self.stats['end_time']}")
        print(f"üìÅ Sources trait√©es: {self.stats['sources_processed']}")
        print(f"üì¶ Enregistrements ing√©r√©s: {self.stats['records_ingested']:,}")
        if self.stats['errors']:
            print(f"‚ùå Erreurs: {len(self.stats['errors'])}")
            for err in self.stats['errors']:
                print(f"   - {err['source']}: {err['error']}")
        print("="*60)


# ============ INTERFACE CLI ============

if __name__ == "__main__":
    import argparse
    
    parser = argparse.ArgumentParser(description='HSE Ingestion pour SafetyGraph')
    parser.add_argument('--source', '-s', help='Source sp√©cifique √† ing√©rer')
    parser.add_argument('--all', '-a', action='store_true', help='Ing√©rer toutes les sources')
    parser.add_argument('--priority', '-p', type=int, help='Filtrer par priorit√© (1, 2, 3)')
    parser.add_argument('--list', '-l', action='store_true', help='Lister les sources')
    parser.add_argument('--config', '-c', default='config/sources.yaml', help='Fichier config')
    
    args = parser.parse_args()
    
    orchestrator = HSEIngestionOrchestrator(config_path=args.config)
    
    if args.list:
        print("\nüìã Sources disponibles:")
        for name, config in orchestrator.config['sources'].items():
            status = "‚úÖ" if config.get('enabled', True) else "‚ùå"
            priority = config.get('priority', '-')
            print(f"  {status} {name} (priorit√©: {priority})")
            
    elif args.source:
        orchestrator.initialize_loaders()
        orchestrator.ingest_source(args.source)
        orchestrator.close_loaders()
        
    elif args.all:
        orchestrator.run_all(priority=args.priority)
        
    else:
        parser.print_help()
```

### √âtape 5 : Cr√©er le DAG Airflow pour Automatisation

Cr√©er `config/airflow/dags/hse_ingestion_dag.py`:

```python
"""
DAG Airflow pour l'ingestion automatis√©e HSE dans SafetyGraph
"""

from datetime import datetime, timedelta
from airflow import DAG
from airflow.operators.python import PythonOperator
from airflow.operators.bash import BashOperator
from airflow.utils.dates import days_ago

# Configuration par d√©faut
default_args = {
    'owner': 'safetygraph',
    'depends_on_past': False,
    'email': ['alerts@safetygraph.ai'],
    'email_on_failure': True,
    'email_on_retry': False,
    'retries': 3,
    'retry_delay': timedelta(minutes=5),
}

# D√©finition du DAG
dag = DAG(
    'hse_ingestion_safetygraph',
    default_args=default_args,
    description='Ingestion donn√©es HSE multi-sources vers SafetyGraph KG',
    schedule_interval='0 2 * * *',  # Daily at 2 AM
    start_date=days_ago(1),
    catchup=False,
    tags=['safetygraph', 'hse', 'ingestion', 'knowledge-graph'],
)

# T√¢che 1: CNESST (priorit√© haute - Qu√©bec)
ingest_cnesst = BashOperator(
    task_id='ingest_cnesst',
    bash_command='cd /opt/safetygraph/etl && python orchestrator.py --source cnesst_opendata',
    dag=dag,
)

# T√¢che 2: OSHA Inspections (priorit√© haute - USA)
ingest_osha_inspections = BashOperator(
    task_id='ingest_osha_inspections',
    bash_command='cd /opt/safetygraph/etl && python orchestrator.py --source osha_inspections',
    dag=dag,
)

# T√¢che 3: OSHA Severe Injuries
ingest_osha_severe = BashOperator(
    task_id='ingest_osha_severe',
    bash_command='cd /opt/safetygraph/etl && python orchestrator.py --source osha_severe_injuries',
    dag=dag,
)

# T√¢che 4: Eurostat (hebdomadaire via trigger)
ingest_eurostat = BashOperator(
    task_id='ingest_eurostat',
    bash_command='cd /opt/safetygraph/etl && python orchestrator.py --source eurostat_esaw',
    dag=dag,
)

# T√¢che 5: Mise √† jour Knowledge Graph
update_kg_stats = BashOperator(
    task_id='update_kg_stats',
    bash_command='''
    cd /opt/safetygraph/etl && python -c "
from loaders.neo4j_kg_loader import Neo4jKGLoader
loader = Neo4jKGLoader()
loader.connect()
stats = loader.get_stats()
print(f'KG Stats: {stats}')
loader.close()
"
    ''',
    dag=dag,
)

# T√¢che 6: Notification aux agents
notify_agents = BashOperator(
    task_id='notify_agents',
    bash_command='curl -X POST http://safetygraph-api:8000/api/agents/refresh-data',
    dag=dag,
)

# D√©finition des d√©pendances
[ingest_cnesst, ingest_osha_inspections, ingest_osha_severe] >> ingest_eurostat
ingest_eurostat >> update_kg_stats >> notify_agents
```

### √âtape 6 : Int√©gration GitLab CI/CD

Ajouter √† votre `.gitlab-ci.yml` existant:

```yaml
# ============ HSE INGESTION PIPELINE ============

stages:
  - test
  - ingest
  - validate
  - notify

# Variables globales
variables:
  HSE_CONFIG_PATH: "safetygraph-data/etl/hse_ingestion/config/sources.yaml"

# Test des connecteurs
test_hse_connectors:
  stage: test
  image: python:3.11
  script:
    - cd safetygraph-data/etl/hse_ingestion
    - pip install -r requirements.txt
    - python -m pytest tests/ -v
  only:
    - merge_requests
    - main
  tags:
    - docker

# Ingestion journali√®re (scheduled)
daily_hse_ingestion:
  stage: ingest
  image: python:3.11
  script:
    - cd safetygraph-data/etl/hse_ingestion
    - pip install -r requirements.txt
    - python orchestrator.py --all --priority 1
  only:
    - schedules
  variables:
    NEO4J_URI: $NEO4J_PRODUCTION_URI
    NEO4J_PASSWORD: $NEO4J_PRODUCTION_PASSWORD
  artifacts:
    reports:
      dotenv: ingestion_stats.env
    paths:
      - logs/
    expire_in: 7 days
  tags:
    - production

# Validation Knowledge Graph
validate_kg_integrity:
  stage: validate
  needs: ["daily_hse_ingestion"]
  script:
    - |
      python -c "
      from loaders.neo4j_kg_loader import Neo4jKGLoader
      loader = Neo4jKGLoader()
      loader.connect()
      stats = loader.get_stats()
      assert stats['incidents'] > 0, 'No incidents in KG'
      print(f'‚úÖ KG Validation passed: {stats}')
      loader.close()
      "
  only:
    - schedules
  tags:
    - production

# Notification Slack
notify_ingestion_complete:
  stage: notify
  needs: ["validate_kg_integrity"]
  script:
    - |
      curl -X POST $SLACK_WEBHOOK_URL \
        -H 'Content-type: application/json' \
        -d '{
          "text": "‚úÖ HSE Ingestion SafetyGraph compl√©t√©e",
          "blocks": [
            {
              "type": "section",
              "text": {
                "type": "mrkdwn",
                "text": "*HSE Ingestion Report*\n‚Ä¢ Sources: CNESST, OSHA, Eurostat\n‚Ä¢ Status: Success"
              }
            }
          ]
        }'
  only:
    - schedules
  tags:
    - production
```

---

## üß™ Tests et Validation

### Script de Test Rapide

```powershell
# test_integration.ps1
# Ex√©cuter dans votre r√©pertoire SafeGraph

Write-Host "üß™ Test d'int√©gration HSE Ingestion ‚Üí SafetyGraph" -ForegroundColor Cyan

# 1. V√©rifier la structure
Write-Host "`n1Ô∏è‚É£ V√©rification structure..." -ForegroundColor Yellow
$paths = @(
    "safetygraph-data/etl/hse_ingestion/orchestrator.py",
    "safetygraph-data/etl/hse_ingestion/config/sources.yaml",
    "safetygraph-data/etl/hse_ingestion/loaders/neo4j_kg_loader.py"
)
foreach ($path in $paths) {
    if (Test-Path $path) {
        Write-Host "  ‚úÖ $path" -ForegroundColor Green
    } else {
        Write-Host "  ‚ùå $path (MANQUANT)" -ForegroundColor Red
    }
}

# 2. Test import Python
Write-Host "`n2Ô∏è‚É£ Test imports Python..." -ForegroundColor Yellow
python -c "
from safetygraph-data.etl.hse_ingestion.orchestrator import HSEIngestionOrchestrator
print('  ‚úÖ Orchestrator import√©')
"

# 3. Test connexion Neo4j
Write-Host "`n3Ô∏è‚É£ Test connexion Neo4j..." -ForegroundColor Yellow
python -c "
import os
os.environ['NEO4J_URI'] = 'bolt://localhost:7687'
from loaders.neo4j_kg_loader import Neo4jKGLoader
try:
    loader = Neo4jKGLoader()
    loader.connect()
    stats = loader.get_stats()
    print(f'  ‚úÖ Neo4j connect√©: {stats}')
    loader.close()
except Exception as e:
    print(f'  ‚ö†Ô∏è Neo4j non disponible: {e}')
"

Write-Host "`n‚úÖ Tests termin√©s" -ForegroundColor Green
```

---

## üìä Commandes Rapides

```bash
# Lister les sources disponibles
python orchestrator.py --list

# Ing√©rer CNESST uniquement
python orchestrator.py --source cnesst_opendata

# Ing√©rer toutes les sources priorit√© 1
python orchestrator.py --all --priority 1

# Ingestion compl√®te
python orchestrator.py --all

# V√©rifier stats KG apr√®s ingestion
python -c "from loaders.neo4j_kg_loader import Neo4jKGLoader; l=Neo4jKGLoader(); l.connect(); print(l.get_stats()); l.close()"
```

---

## üéØ R√©sultat Final

Apr√®s int√©gration compl√®te:

| M√©trique | Avant | Apr√®s |
|----------|-------|-------|
| Sources connect√©es | 3 | 15+ |
| Volume donn√©es KG | 22M | 80M+ |
| Couverture g√©ographique | 2 juridictions | 180+ pays |
| Fr√©quence mise √† jour | Manuel | Automatique (daily) |
| Agents aliment√©s | Partiel | 110+ (complet) |

---

*Guide d'int√©gration SafetyGraph - HSE Ingestion Package*
*Version 1.0 - Janvier 2026*
